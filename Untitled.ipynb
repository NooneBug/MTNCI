{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../figet-hyperbolic-space/')\n",
    "import figet\n",
    "from figet.model_utils import CharEncoder, SelfAttentiveSum, sort_batch_by_length\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabs': {'token': <figet.Dict.TokenDict at 0x7fa858791c10>,\n",
       "  'type': <figet.Dict.TypeDict at 0x7fa77c45e160>,\n",
       "  'char': <figet.Dict.Dict at 0x7fa77c45e190>},\n",
       " 'train': <figet.Dataset.Dataset at 0x7fa77c45e1c0>,\n",
       " 'dev': <figet.Dataset.Dataset at 0x7fa8587abbe0>,\n",
       " 'test': <figet.Dataset.Dataset at 0x7fa77c45e3a0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lopez_data = torch.load('../figet-hyperbolic-space/data/prep/MTNCI/data.pt')\n",
    "lopez_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.add_argument(\"--emb_size\", default=300, type=int, help=\"Embedding size.\")\n",
    "# parser.add_argument(\"--char_emb_size\", default=50, type=int, help=\"Char embedding size.\")\n",
    "# parser.add_argument(\"--positional_emb_size\", default=25, type=int, help=\"Positional embedding size.\")\n",
    "# parser.add_argument(\"--context_rnn_size\", default=200, type=int, help=\"RNN size of ContextEncoder.\")\n",
    "# parser.add_argument(\"--attn_size\", default=100, type=int, help=\"Attention vector size.\")\n",
    "# parser.add_argument(\"--mention_dropout\", default=0.5, type=float, help=\"Dropout rate for mention\")\n",
    "# parser.add_argument(\"--context_dropout\", default=0.2, type=float, help=\"Dropout rate for context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "\n",
    "class CharEncoder(nn.Module):\n",
    "    def __init__(self, char_vocab, args):\n",
    "        super(CharEncoder, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        conv_dim_input = 100\n",
    "        filters = 5\n",
    "        self.char_W = nn.Embedding(char_vocab.size(), conv_dim_input, padding_idx=0)\n",
    "        self.conv1d = nn.Conv1d(conv_dim_input, args.char_emb_size, filters)  # input, output, filter_number\n",
    "\n",
    "    def forward(self, span_chars):\n",
    "        char_embed = self.char_W(span_chars).transpose(1, 2)  # [batch_size, char_embedding, max_char_seq]\n",
    "        conv_output = [self.conv1d(char_embed)]  # list of [batch_size, filter_dim, max_char_seq, filter_number]\n",
    "        conv_output = [F.relu(c) for c in conv_output]  # batch_size, filter_dim, max_char_seq, filter_num\n",
    "        cnn_rep = [F.max_pool1d(i, i.size(2)) for i in conv_output]  # batch_size, filter_dim, 1, filter_num\n",
    "        cnn_output = torch.squeeze(torch.cat(cnn_rep, 1), 2)  # batch_size, filter_num * filter_dim, 1\n",
    "        return cnn_output\n",
    "\n",
    "class MentionEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, char_vocab, args):\n",
    "        super(MentionEncoder, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.char_encoder = CharEncoder(char_vocab, args)\n",
    "        self.attentive_weighted_average = SelfAttentiveSum(args.emb_size, 1)\n",
    "        self.dropout = nn.Dropout(args.mention_dropout)\n",
    "\n",
    "    def forward(self, mentions, mention_chars, word_lut):\n",
    "        mention_embeds = word_lut(mentions)             # batch x mention_length x emb_size\n",
    "\n",
    "        weighted_avg_mentions, _ = self.attentive_weighted_average(mention_embeds)\n",
    "        char_embed = self.char_encoder(mention_chars)\n",
    "        output = torch.cat((weighted_avg_mentions, char_embed), 1)\n",
    "        return self.dropout(output).cuda()\n",
    "\n",
    "\n",
    "class ContextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.emb_size = args.emb_size\n",
    "        self.pos_emb_size = args.positional_emb_size\n",
    "        self.rnn_size = args.context_rnn_size\n",
    "        self.hidden_attention_size = 100\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.pos_linear = nn.Linear(1, self.pos_emb_size)\n",
    "        self.context_dropout = nn.Dropout(args.context_dropout)\n",
    "        self.rnn = nn.LSTM(self.emb_size + self.pos_emb_size, self.rnn_size, bidirectional=True, batch_first=True)\n",
    "        self.attention = SelfAttentiveSum(self.rnn_size * 2, self.hidden_attention_size) # x2 because of bidirectional\n",
    "\n",
    "    def forward(self, contexts, positions, context_len, word_lut, hidden=None):\n",
    "        \"\"\"\n",
    "        :param contexts: batch x max_seq_len\n",
    "        :param positions: batch x max_seq_len\n",
    "        :param context_len: batch x 1\n",
    "        \"\"\"\n",
    "        positional_embeds = self.get_positional_embeddings(positions)   # batch x max_seq_len x pos_emb_size\n",
    "        ctx_word_embeds = word_lut(contexts)                            # batch x max_seq_len x emb_size\n",
    "        ctx_embeds = torch.cat((ctx_word_embeds, positional_embeds), 2)\n",
    "\n",
    "        ctx_embeds = self.context_dropout(ctx_embeds)\n",
    "\n",
    "        rnn_output = self.sorted_rnn(ctx_embeds, context_len)\n",
    "\n",
    "        return self.attention(rnn_output)\n",
    "\n",
    "    def get_positional_embeddings(self, positions):\n",
    "        \"\"\" :param positions: batch x max_seq_len\"\"\"\n",
    "        pos_embeds = self.pos_linear(positions.view(-1, 1))                     # batch * max_seq_len x pos_emb_size\n",
    "        return pos_embeds.view(positions.size(0), positions.size(1), -1)        # batch x max_seq_len x pos_emb_size\n",
    "\n",
    "    def sorted_rnn(self, ctx_embeds, context_len):\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices = sort_batch_by_length(ctx_embeds, context_len)\n",
    "        packed_sequence_input = pack(sorted_inputs, sorted_sequence_lengths, batch_first=True)\n",
    "        packed_sequence_output, _ = self.rnn(packed_sequence_input, None)\n",
    "        unpacked_sequence_tensor, _ = unpack(packed_sequence_output, batch_first=True)\n",
    "        return unpacked_sequence_tensor.index_select(0, restoration_indices)\n",
    "\n",
    "\n",
    "\n",
    "# def get_shimaoka(input, mention_encoder, context_encoder):\n",
    "#     contexts, positions, context_len = input[0], input[1], input[2]\n",
    "#     mentions, mention_chars = input[3], input[4]\n",
    "#     type_indexes = input[5]\n",
    "\n",
    "#     mention_vec = mention_encoder(mentions, mention_chars, self.word_lut)\n",
    "#     context_vec, attn = context_encoder(contexts, positions, context_len, self.word_lut)\n",
    "\n",
    "#     input_vec = torch.cat((mention_vec, context_vec), dim=1)\n",
    "#     return input_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MTNCI import MTNCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argClass():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.emb_size = 300 \n",
    "        self.char_emb_size = 50 \n",
    "        self.positional_emb_size = 25 \n",
    "        self.context_rnn_size = 200\n",
    "        self.attn_size = 100\n",
    "        self.mention_dropout = 0.5\n",
    "        self.context_dropout = 0.5\n",
    "\n",
    "\n",
    "args = {'emb_size': 300, 'char_emb_size': 50, 'positional_emb_size': 25, 'context_rnn_size':200,\n",
    "        'attn_size': 100, 'mention_dropout' : 0.5, 'context_dropout': 0.5}\n",
    "args = argClass(args)\n",
    "vocabs = lopez_data['vocabs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShimaokaMTNCI(MTNCI):\n",
    "    \n",
    "    def __init__(self, argss, vocabs, device, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        CHAR_VOCAB = 'char'        \n",
    "        self.word_lut = nn.Embedding(vocabs[\"token\"].size_of_word2vecs(), \n",
    "                                     argss.emb_size,\n",
    "                                     padding_idx=0).cuda()\n",
    "        \n",
    "        self.mention_encoder = MentionEncoder(vocabs[CHAR_VOCAB], argss).cuda()\n",
    "        self.context_encoder = ContextEncoder(argss).cuda()\n",
    "        self.feature_len = argss.context_rnn_size * 2 + argss.emb_size + argss.char_emb_size\n",
    "    \n",
    "    def init_params(self, word2vec):\n",
    "        self.word_lut.weight.data.copy_(word2vec)\n",
    "        self.word_lut.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, input):\n",
    "        contexts, positions, context_len = input[0], input[1].double(), input[2]\n",
    "        mentions, mention_chars = input[3], input[4]\n",
    "        type_indexes = input[5]\n",
    "                \n",
    "        mention_vec = self.mention_encoder(mentions, mention_chars, self.word_lut)\n",
    "        \n",
    "        context_vec, attn = self.context_encoder(contexts, positions, context_len, self.word_lut)\n",
    "\n",
    "        input_vec = torch.cat((mention_vec, context_vec), dim=1)\n",
    "        \n",
    "        return super().forward(input_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIMAOKA_OUT = args.context_rnn_size * 2 + args.emb_size + args.char_emb_size\n",
    "\n",
    "out_spec = [{'manifold':'euclid', 'dim':[64, 10]},\n",
    "                {'manifold':'poincare', 'dim':[128, 128, 10]}]\n",
    "\n",
    "m = ShimaokaMTNCI(args, vocabs, device, \n",
    "                  input_d=SHIMAOKA_OUT,\n",
    "                out_spec = out_spec,\n",
    "                dims = [512, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size, key):\n",
    "    dataset = data[key]\n",
    "    dataset.set_batch_size(batch_size)\n",
    "    return dataset\n",
    "\n",
    "test = get_dataset(lopez_data, 1024, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = torch.load(\"../figet-hyperbolic-space/data/prep/MTNCI/word2vec.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.init_params(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabs': {'token': <figet.Dict.TokenDict at 0x7fa858791c10>,\n",
       "  'type': <figet.Dict.TypeDict at 0x7fa77c45e160>,\n",
       "  'char': <figet.Dict.Dict at 0x7fa77c45e190>},\n",
       " 'train': <figet.Dataset.Dataset at 0x7fa77c45e1c0>,\n",
       " 'dev': <figet.Dataset.Dataset at 0x7fa8587abbe0>,\n",
       " 'test': <figet.Dataset.Dataset at 0x7fa77c45e3a0>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lopez_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MTNCI_figet]",
   "language": "python",
   "name": "conda-env-MTNCI_figet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
