{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('./preprocessing/')\n",
    "from utils import load_data_with_pickle, save_data_with_pickle\n",
    "import numpy as np\n",
    "from DatasetManager import DatasetManager\n",
    "\n",
    "EMBEDDING_PATH = '../source_files/embeddings/'\n",
    "\n",
    "PATH_TO_HYPERBOLIC_EMBEDDING = EMBEDDING_PATH + '12_3_v2_final_tree_HyperE_MTNCI'\n",
    "\n",
    "PATH_TO_DISTRIBUTIONAL_EMBEDDING = EMBEDDING_PATH + '12_3_v2_type2vec_MTNCI'\n",
    "# PATH_TO_DISTRIBUTIONAL_EMBEDDING = EMBEDDING_PATH + 'FEDE_type2vec'\n",
    "\n",
    "\n",
    "CONCEPT_EMBEDDING_PATHS = [PATH_TO_DISTRIBUTIONAL_EMBEDDING, PATH_TO_HYPERBOLIC_EMBEDDING]\n",
    "\n",
    "DATASET_PATH = '../source_files/vectors/'\n",
    "\n",
    "X_PATH = DATASET_PATH + '12_3_v2_X'\n",
    "Y_PATH = DATASET_PATH + '12_3_v2_Y'\n",
    "ENTITIES_PATH = DATASET_PATH + '12_3_v2_entities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings = [load_data_with_pickle(x) for x in CONCEPT_EMBEDDING_PATHS]\n",
    "concept_embeddings = {'hyperbolic': concept_embeddings[1], \n",
    "                      'distributional':concept_embeddings[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmanuel/.conda/envs/MTNCI/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "concept_embeddings['distributional'] = {k: concept_embeddings['distributional'][k] for k in concept_embeddings['distributional'].wv.vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings['hyperbolic'] = concept_embeddings['hyperbolic'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_data_with_pickle(X_PATH)\n",
    "Y = load_data_with_pickle(Y_PATH)\n",
    "entities = load_data_with_pickle(ENTITIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabel alerts: 0\n",
      "Multilabel alerts: 0\n"
     ]
    }
   ],
   "source": [
    "from CorpusManager import CorpusManager\n",
    "\n",
    "c = CorpusManager()\n",
    "\n",
    "f_e_d = load_data_with_pickle('../source_files/pickles/12_3_v2_found_entity_dict')\n",
    "tree = load_data_with_pickle('../source_files/pickles/12_3_v2_final_tree')\n",
    "\n",
    "from collections import defaultdict\n",
    "reverse_dict = defaultdict(list)\n",
    "\n",
    "for k, words in f_e_d.items():\n",
    "    for w in words:\n",
    "        reverse_dict[w].append(k)\n",
    "\n",
    "found_entity_dict = c.avoid_multilabeling(f_e_d, tree, file = '../source_files/logs/avoid_multilabeling.txt')\n",
    "\n",
    "reverse_dict = defaultdict(list)\n",
    "\n",
    "for k, words in found_entity_dict.items():\n",
    "    for w in words:\n",
    "        reverse_dict[w].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1643818"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M'],\n",
       " ['M']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1643818"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-08a30706aa2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnot_present\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     for emb in concept_embeddings.values():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "not_present = []\n",
    "\n",
    "for label in list(set(Y)):\n",
    "#     for emb in concept_embeddings.values():\n",
    "    try:\n",
    "        a = concept_embeddings['distributional'][label]\n",
    "    except:\n",
    "        if label not in not_present:\n",
    "            not_present.append(label)\n",
    "len(not_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x, y in zip(X, Y) if y not in not_present]\n",
    "entities = [e for e, y in zip(entities, Y) if y not in not_present]\n",
    "Y = [y for y in Y if y not in not_present]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 1171950, Y : 1171950, ENTITIES :1171950\n"
     ]
    }
   ],
   "source": [
    "print('X: {}, Y : {}, ENTITIES :{}'.format(len(X), len(Y), len(entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "class ciaone:\n",
    "    def split_data_by_unique_entities(self, X, Y, entities, exclude_min_threshold = 3):\n",
    "            self.unique_entities = list(set(entities))\n",
    "            self.direct_labeling = {e: y for e, y in zip(entities, Y)}\n",
    "            self.labels_of_unique_entities = [self.direct_labeling[e] for e in self.unique_entities]\n",
    "            \n",
    "            counter = Counter(self.labels_of_unique_entities)\n",
    "            \n",
    "            # exclude labels with less than 3 elements, these will not be useful\n",
    "            \n",
    "            self.filtered_entities = [e for e, l in zip(self.unique_entities, self.labels_of_unique_entities) if counter[l] >= exclude_min_threshold]\n",
    "            self.filtered_labels = [l for l in self.labels_of_unique_entities if counter[l] >= exclude_min_threshold]\n",
    "            self.filtered_out = [l for l in self.labels_of_unique_entities if counter[l] < exclude_min_threshold]\n",
    "            \n",
    "            self.filtered_out_len = len(set(self.filtered_out))\n",
    "            \n",
    "            print('{} labels filtered out based on exclude_min_threshold ({:.2f}% on dataset dimension)'.format(self.filtered_out_len,\n",
    "                                                                                               len(self.filtered_out)/len(self.labels_of_unique_entities)))\n",
    "            print('Initial labels: {}, current labels: {}'.format(len(set(self.labels_of_unique_entities)),\n",
    "                                                                  len(set(self.filtered_labels))))\n",
    "            self.entities_train, self.entities_test, \\\n",
    "                self.y_train_split, self.y_test_split = train_test_split(self.filtered_entities,\n",
    "                                                                         self.filtered_labels,\n",
    "                                                                         test_size = 0.1,\n",
    "                                                                         stratify = self.filtered_labels)\n",
    "            \n",
    "            self.entities_train, self.entities_val, \\\n",
    "                self.y_train, self.y_val = train_test_split(self.entities_train,\n",
    "                                                  self.y_train_split,\n",
    "                                                  test_size = 0.1,\n",
    "                                                  stratify = self.y_train_split)\n",
    "            \n",
    "            self.X_train = []\n",
    "            self.Y_train = []\n",
    "            self.E_train = []\n",
    "\n",
    "            self.X_val = []\n",
    "            self.Y_val = []\n",
    "            self.E_val = []\n",
    "\n",
    "            self.X_test = []\n",
    "            self.Y_test = []\n",
    "            self.E_test = []\n",
    "            \n",
    "            bar = tqdm(total = len(X))\n",
    "            filtered_set = set(self.filtered_entities)\n",
    "            for x, y, e in zip(X, Y, entities):\n",
    "                bar.update(1)\n",
    "                if e in filtered_set:\n",
    "                    if e in self.entities_train:\n",
    "                        self.X_train.append(x)\n",
    "                        self.Y_train.append(y)        \n",
    "                        self.E_train.append(e)\n",
    "\n",
    "                    elif e in self.entities_test:\n",
    "                        self.X_test.append(x)\n",
    "                        self.Y_test.append(y)\n",
    "                        self.E_test.append(e)\n",
    "\n",
    "                    elif e in self.entities_val:\n",
    "                        self.X_val.append(x)\n",
    "                        self.Y_val.append(y)\n",
    "                        self.E_val.append(e)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 labels filtered out based on exclude_min_threshold (0.29% on dataset dimension)\n",
      "Initial labels: 339, current labels: 54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfc9696981043cfa7afdd616f781dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1171950.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = ciaone()\n",
    "c.split_data_by_unique_entities(X, Y, entities, exclude_min_threshold = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 645047, Val : 75032, Test :101938\n"
     ]
    }
   ],
   "source": [
    "print('Train: {}, Val : {}, Test :{}'.format(len(c.Y_train), len(c.Y_val), len(c.Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_dataset_and_sample(X, Y, E, fraction = 1):\n",
    "    random.seed(236451)\n",
    "    frac = int(len(X) * fraction) \n",
    "    A = [[x, y, e] for x, y, e in zip(X, Y, E)]\n",
    "    random.shuffle(A)\n",
    "    return [x[0] for i, x in enumerate(A) if i < frac], [x[1] for i, x in enumerate(A) if i < frac], [x[2] for i, x in enumerate(A) if i < frac]\n",
    "\n",
    "c.X_train, c.Y_train, c.E_train = shuffle_dataset_and_sample(c.X_train, c.Y_train, c.E_train, 0.10)\n",
    "\n",
    "c.X_test, c.Y_test, c.E_test = shuffle_dataset_and_sample(c.X_test, c.Y_test, c.E_test, 0.10)\n",
    "\n",
    "c.X_val, c.Y_val, c.E_val = shuffle_dataset_and_sample(c.X_val, c.Y_val, c.E_val, 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Train              |               Val               |              Test               \n",
      "---------------------------------|---------------------------------|---------------------------------\n",
      "          Place           14234  |          Place            1610   |          Place            2051   \n",
      "      Organisation         5262  |         Protein           1038   |         Device            1315   \n",
      "         Person            4739  |      MilitaryUnit          654   |      Organisation          829   \n",
      "       Settlement          3572  |      Organisation          624   |          Band              806   \n",
      "       MusicalWork         2351  |         Person             369   |         Company            731   \n",
      "         Company           2337  |         Species            287   |         Person             516   \n",
      " ArchitecturalStructure    2283  |       MusicalWork          280   |       Automobile           491   \n",
      "         Protein           2077  |         Disease            267   |          Event             361   \n",
      "     TelevisionShow        1490  |   AnatomicalStructure      242   |        Software            265   \n",
      "          Food             1450  |        Election            212   |       WrittenWork          262   \n",
      "        Software           1409  |       WrittenWork          208   |          Book              229   \n",
      "         Species           1398  |         Company            149   |       Settlement           219   \n",
      "   AnatomicalStructure     1363  |        Building            135   |       MusicalWork          206   \n",
      "         Device            1233  |          Book              128   |          City              199   \n",
      "          Band             1197  |       RecordLabel          123   |         Species            141   \n",
      "          Plant            1143  |         Country            119   |        Building            127   \n",
      "      MilitaryUnit         1055  |   FictionalCharacter       103   |        Language            124   \n",
      "          Book             1029  |          Band              88    |         Disease            114   \n",
      "        Building           991   |          Film              84    |          Award             98    \n",
      "         Disease           979   |         Device             80    |       SportsEvent          97    \n",
      "        Election           939   |          Food              75    | ArchitecturalStructure     95    \n",
      "        Language           910   |       Settlement           65    |       SportsTeam           92    \n",
      "          Film             827   |        Mountain            62    |         Protein            88    \n",
      "    ChemicalCompound       808   |        Software            59    |    ChemicalCompound        77    \n",
      "         Country           799   |         Artist             49    |        Mountain            63    \n",
      "          Ship             792   |          City              45    |          Food              62    \n",
      "       WrittenWork         731   |        VideoGame           30    |        Election            46    \n",
      "     PoliticalParty        691   |    ChemicalCompound        26    |   AnatomicalStructure      45    \n",
      "       SportsEvent         668   |         Village            24    |       EthnicGroup          40    \n",
      "         Mammal            525   | ArchitecturalStructure     23    |          Ship              40    \n",
      "       RecordLabel         514   |        Language            23    |      ProtectedArea         35    \n",
      "       Automobile          487   |          Town              23    |         Artist             29    \n",
      "        VideoGame          480   |       Automobile           22    |     TelevisionShow         27    \n",
      "       EthnicGroup         392   |     TelevisionShow         20    |         Country            27    \n",
      "          City             379   |       SportsTeam           18    |         Village            23    \n",
      "          Event            372   |       EthnicGroup          16    |         Animal             23    \n",
      "   FictionalCharacter      342   |          Plant             14    |          Town              22    \n",
      "         Athlete           303   |         Island             12    |         Mammal             20    \n",
      "          Town             265   |          Award             11    |          Film              20    \n",
      "         Artist            181   |         Animal             11    |          Plant             17    \n",
      "      SportsLeague         171   |         Athlete            10    | EducationalInstitution     12    \n",
      "         Island            160   |      OfficeHolder           8    |     PoliticalParty         12    \n",
      "         Village           157   |         Writer              7    |      MilitaryUnit          12    \n",
      "      OfficeHolder         152   |        Scientist            7    |      OfficeHolder          12    \n",
      "          Award            132   | EducationalInstitution      6    |        VideoGame           11    \n",
      "        Mountain           129   |         Mammal              6    |         Athlete            11    \n",
      "       SportsTeam          126   |       Politician            6    |   FictionalCharacter       11    \n",
      "         Animal            103   |          Ship               5    |        Scientist            9    \n",
      "      ProtectedArea         83   |      ProtectedArea          5    |         Writer              8    \n",
      "         Writer             82   |       SportsEvent           5    |         Island              8    \n",
      " EducationalInstitution     69   |          Event              3    |          Fish               4    \n",
      "        Scientist           57   |      SportsLeague           3    |       Politician            4    \n",
      "       Politician           49   |          Fish               2    |       RecordLabel           4    \n",
      "          Fish              37   |     PoliticalParty          2    |      SportsLeague           3    \n"
     ]
    }
   ],
   "source": [
    "Tr = Counter(c.Y_train).most_common()\n",
    "Va = Counter(c.Y_val).most_common()\n",
    "Te = Counter(c.Y_test).most_common()\n",
    "\n",
    "print('{:^33}|{:^33}|{:^33}'.format('Train','Val', 'Test'))\n",
    "print('{:-^33}|{:-^33}|{:-^33}'.format('', '', ''))\n",
    "\n",
    "for x, y, z in zip(Tr, Va, Te):\n",
    "    print('{:^25}{:^8}|{:^25}{:^9}|{:^25}{:^9}'.format(x[0], x[1], y[0], y[1], z[0], z[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PICKLE_PATH = '../source_files/datasets/'\n",
    "\n",
    "ID = '_10_4_300_0.1'\n",
    "\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_X_train' + ID, c.X_train)\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_X_val' + ID, c.X_val)\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_X_test' + ID, c.X_test)\n",
    "\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_Y_train' + ID, c.Y_train)\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_Y_val' + ID, c.Y_val)\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_Y_test' + ID, c.Y_test)\n",
    "\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_entities_train' + ID, c.E_train)\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_entities_val' + ID, c.E_val)\n",
    "save_data_with_pickle(PICKLE_PATH + 'filtered_entities_test' + ID, c.E_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numeric_label(labels):\n",
    "    return [numeric_label_map[y] for y in labels]\n",
    "\n",
    "numeric_label_map = {y: x for x, y in enumerate(set(c.Y_train))}\n",
    "inverse_numeric_label_map = {v:k for k,v in numeric_label_map.items()}\n",
    "\n",
    "len(numeric_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.y_numeric_train = get_numeric_label(c.Y_train)\n",
    "c.y_numeric_test = get_numeric_label(c.Y_test)\n",
    "c.y_numeric_val = get_numeric_label(c.Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dataset for deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_explicit_dataset(y_train, y_test, y_val, embeddings):\n",
    "\n",
    "    explicit_y_train = {k:[] for k in embeddings.keys()}\n",
    "    explicit_y_test = {k:[] for k in embeddings.keys()}\n",
    "    explicit_y_val = {k:[] for k in embeddings.keys()}\n",
    "\n",
    "    for label in y_train:\n",
    "        for k, emb in embeddings.items():\n",
    "            explicit_y_train[k].append(emb[label])\n",
    "    \n",
    "    for label in y_test:\n",
    "        for k, emb in embeddings.items():\n",
    "            explicit_y_test[k].append(emb[label])\n",
    "\n",
    "    for label in y_val:\n",
    "        for k, emb in embeddings.items():\n",
    "            explicit_y_val[k].append(emb[label])\n",
    "    \n",
    "    \n",
    "    for k, dataSET in explicit_y_train.items():\n",
    "        explicit_y_train[k] = np.array(dataSET)\n",
    "    \n",
    "    for k, dataSET in explicit_y_test.items():\n",
    "        explicit_y_test[k] = np.array(dataSET)\n",
    "    \n",
    "    for k, dataSET in explicit_y_val.items():\n",
    "        explicit_y_val[k] = np.array(dataSET)\n",
    "        \n",
    "    return explicit_y_train, explicit_y_test, explicit_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_y_train, explicit_y_test, explicit_y_val = create_explicit_dataset(c.Y_train, c.Y_test, c.Y_val, concept_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, vector_list, label_list, target_list):\n",
    "        self.X = torch.tensor(vector_list, device = device)\n",
    "        self.labels = torch.tensor(label_list, device = device)\n",
    "        self.target_list = {k: torch.tensor(x, dtype = torch.float64, device = device) for k, x in target_list.items()}\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index], \n",
    "                self.labels[index],\n",
    "                {k:x[index] for k, x in self.target_list.items()}\n",
    "               )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MyDataset(c.X_train, \n",
    "                     c.y_numeric_train, \n",
    "                     explicit_y_train) \n",
    "trainloader = DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "testset = MyDataset(c.X_test,\n",
    "                    c.y_numeric_test,\n",
    "                    explicit_y_test) \n",
    "testloader = DataLoader(testset, batch_size=512, shuffle=False)\n",
    "\n",
    "valset = MyDataset(c.X_val, \n",
    "                   c.y_numeric_val, \n",
    "                   explicit_y_val)\n",
    "valloader = DataLoader(valset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_losses(epoch, epochs, epochs_no_improve, n_epochs_stop, loss, val_loss, sim, colored, val_colored, sim_colored, norms, time):\n",
    "    print(Fore.WHITE + \"Epoch: {:>2}/{}..\".format(epoch+1, epochs))\n",
    "    \n",
    "    names = ['Distributional', 'Hyperbolic']\n",
    "    \n",
    "    print(Fore.WHITE + \"{}{:>14} {:>12} \".format(names[0], \n",
    "                                                    ' Training Loss:', Fore.GREEN + str(round(loss[0], 5)) if colored[0] \n",
    "                                                    else Fore.WHITE + str(round(loss[0], 5))),\n",
    "          Fore.WHITE + \"{}{:>9} {:>12} \".format(names[0], ' Val Loss:', Fore.GREEN + str(round(val_loss[0], 5)) if val_colored[0] \n",
    "                                                    else Fore.WHITE + str(round(val_loss[0], 5))),\n",
    "          Fore.WHITE + \"{}{:>14} {:>12} \".format(names[1], \n",
    "                                                    ' Training Loss:', Fore.GREEN + str(round(loss[1], 5)) if colored[1] \n",
    "                                                    else Fore.WHITE + str(round(loss[1], 5))),\n",
    "          Fore.WHITE + \"{}{:>9} {:>12} \\n\".format(names[1], ' Val Loss:', Fore.GREEN + str(round(val_loss[1], 5)) if val_colored[1] \n",
    "                                                    else Fore.WHITE + str(round(val_loss[1], 5))),\n",
    "          Fore.WHITE + \"Mean Similarities: {:>16} \".format(Fore.GREEN + str(round(sim[0], 4)) if sim_colored[0]\n",
    "                                                           else Fore.WHITE + str(round(sim[0], 4))),\n",
    "          Fore.WHITE + \"{:>16}\".format(Fore.GREEN + str(round(sim[1], 4)) if sim_colored[1]\n",
    "                                        else Fore.WHITE + str(round(sim[1], 4))),\n",
    "          Fore.WHITE + \"  Val Loss: {:>16} \".format(Fore.GREEN + str(round(val_loss[2], 5)) if val_colored[2]\n",
    "                                                           else Fore.WHITE + str(round(val_loss[2], 5))),\n",
    "          Fore.WHITE + \" Epochs from last best: {}/{}\".format(epochs_no_improve, n_epochs_stop),\n",
    "          Fore.WHITE + \"Mean Norm: {}\".format(round(norms, 5)),\n",
    "          Fore.WHITE + \"Epoch Time : {}\".format(round(time, 4))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_loss(true, pred):\n",
    "    cossim = torch.nn.CosineSimilarity(dim = 1)\n",
    "    return 1 - cossim(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoopt\n",
    "class MobiusLinear(torch.nn.Linear):\n",
    "    def __init__(self, *args, nonlin=None, ball=None, c=1.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # for manifolds that have parameters like Poincare Ball\n",
    "        # we have to attach them to the closure Module.\n",
    "        # It is hard to implement device allocation for manifolds in other case.\n",
    "        self.ball = create_ball(ball, c)\n",
    "        if self.bias is not None:\n",
    "            self.bias = geoopt.ManifoldParameter(self.bias, manifold=self.ball)\n",
    "        self.nonlin = nonlin\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return mobius_linear(\n",
    "            input,\n",
    "            weight=self.weight,\n",
    "            bias=self.bias,\n",
    "            nonlin=self.nonlin,\n",
    "            ball=self.ball,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.eye_(self.weight)\n",
    "        self.weight.add_(torch.rand_like(self.weight).mul_(1e-3))\n",
    "        if self.bias is not None:\n",
    "            self.bias.zero_()\n",
    "\n",
    "\n",
    "# package.nn.functional.py\n",
    "def mobius_linear(input, weight, bias=None, nonlin=None, *, ball: geoopt.PoincareBall):\n",
    "    output = ball.mobius_matvec(weight, input)\n",
    "    if bias is not None:\n",
    "        output = ball.mobius_add(output, bias)\n",
    "    if nonlin is not None:\n",
    "        output = ball.logmap0(output)\n",
    "        output = nonlin(output)\n",
    "        output = ball.expmap0(output)\n",
    "    return output\n",
    "\n",
    "def create_ball(ball=None, c=None):\n",
    "    \"\"\"\n",
    "    Helper to create a PoincareBall.\n",
    "    Sometimes you may want to share a manifold across layers, e.g. you are using scaled PoincareBall.\n",
    "    In this case you will require same curvature parameters for different layers or end up with nans.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ball : geoopt.PoincareBall\n",
    "    c : float\n",
    "    Returns\n",
    "    -------\n",
    "    geoopt.PoincareBall\n",
    "    \"\"\"\n",
    "    if ball is None:\n",
    "        assert c is not None, \"curvature of the ball should be explicitly specified\"\n",
    "        ball = geoopt.PoincareBall(c)\n",
    "    # else trust input\n",
    "    return ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CommonLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_d,\n",
    "                 dims = None,\n",
    "                 dropout_prob = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        prec = input_d\n",
    "        self.fully = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        for dim in dims:\n",
    "            self.fully.append(nn.Linear(prec, dim).cuda())\n",
    "            self.bns.append(nn.BatchNorm1d(dim).cuda())\n",
    "            prec = dim            \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_prob).cuda()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.fully)):\n",
    "            x = x.double()\n",
    "            x = self.dropout(self.bns[i](self.leaky_relu(self.fully[i](x))))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, hidden_dim, dims, manifold):\n",
    "        super().__init__()\n",
    "        self.out = nn.ModuleList()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1).cuda()\n",
    "        self.leaky_relu = nn.ReLU().cuda()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        prec = hidden_dim\n",
    "        \n",
    "        for dim in dims:\n",
    "            if manifold == 'poincare':\n",
    "                self.out.append(MobiusLinear(prec, dim).cuda())\n",
    "            elif manifold == 'euclid':\n",
    "                self.out.append(nn.Linear(prec, dim).cuda())\n",
    "            else:\n",
    "                print('ERROR: NO MODE SELECTED')\n",
    "                \n",
    "            self.bns.append(nn.BatchNorm1d(dim).cuda())\n",
    "            prec = dim\n",
    "            \n",
    "    def forward(self, x):\n",
    "#         x = x.double()\n",
    "        for i in range(len(self.out) - 1):\n",
    "            x = self.leaky_relu(self.out[i](x))\n",
    "        out = self.out[-1](x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTNCI(nn.Module):    \n",
    "    def __init__(self,\n",
    "                 input_d, \n",
    "                 dims = None,\n",
    "                 out_spec = [{'manifold':'euclid', 'dim':[10]},\n",
    "                             {'manifold':'poincare', 'dim':[2]}],\n",
    "                 dropout_prob = 0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.common_network = CommonLayer(input_d=input_d,\n",
    "                                          dims = dims,\n",
    "                                          dropout_prob=dropout_prob)\n",
    "        \n",
    "        self.out_layers = nn.ModuleList()\n",
    "        \n",
    "        for spec in out_spec:\n",
    "            self.out_layers.append(RegressionOutput(hidden_dim=dims[-1],\n",
    "                                              dims=spec['dim'],\n",
    "                                              manifold = spec['manifold']))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.common_network(x)\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for layer in self.out_layers:\n",
    "            output.append(layer(x))\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acosh(x):\n",
    "    return torch.log(x + torch.sqrt(x**2-1))\n",
    "\n",
    "def mse(y_pred, y_true, regul):    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    return mse_loss(y_pred, y_true)\n",
    "\n",
    "\n",
    "def hyperbolic_loss(y_pred, y_true, regul):\n",
    "    numerator = 2 * torch.norm(y_true - y_pred, dim = 1)**2\n",
    "\n",
    "    pred_norm = torch.norm(y_pred, dim = 1)**2\n",
    "    true_norm = torch.norm(y_true, dim = 1)**2\n",
    "\n",
    "    left_denom = 1 - pred_norm\n",
    "    right_denom = 1 - true_norm\n",
    "    \n",
    "#     left_denom = torch.where(left_denom <= 0, torch.tensor(1e-5), left_denom)\n",
    "    \n",
    "    denom = left_denom * right_denom\n",
    "\n",
    "    frac = numerator/denom\n",
    "    acos = acosh(1  + frac)\n",
    "    \n",
    "    \n",
    "    l0 = torch.tensor(1., device = device)\n",
    "    l1 = torch.tensor(1., device = device)\n",
    "    \n",
    "    if sum(regul) > 1:\n",
    "        \n",
    "#         regularization = torch.nn.modules.distance.PairwiseDistance()(y_pred, y_true)\n",
    "        \n",
    "#         regularization = torch.sqrt(numerator/2)\n",
    "        \n",
    "        true_perm = y_true[torch.randperm(y_true.size()[0])]\n",
    "        \n",
    "        l0 = torch.abs(hyperbolic_loss(y_pred, true_perm, regul=[0, 0, 1])[0] - hyperbolic_loss(y_true, true_perm, regul = [0, 0, 1])[0])\n",
    "        l1 = mse(y_pred, y_true, 0)\n",
    "    \n",
    "    return acos**regul[2] + l0 * regul[0] + l1 * regul[1], l0 * regul[0] + l1 * regul[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoopt.optim import RiemannianAdam\n",
    "\n",
    "out_spec = [{'manifold':'euclid', 'dim':[64, len(explicit_y_train['distributional'][0])]},\n",
    "            {'manifold':'poincare', 'dim':[128, len(explicit_y_train['hyperbolic'][0])]}]\n",
    "\n",
    "\n",
    "model = MTNCI(input_d=len(c.X_train[0]),\n",
    "              out_spec = out_spec,\n",
    "              dims = [256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-3,\n",
    "       0]\n",
    "\n",
    "optimizer = RiemannianAdam(model.parameters(), lr = lrs[0])\n",
    "regul = [0, 0, 2]\n",
    "llambda = 0.05\n",
    "\n",
    "\n",
    "LOSSES = {'distributional' : cosine_loss, \n",
    "          'hyperbolic' : hyperbolic_loss}\n",
    "weighted = False\n",
    "\n",
    "min_loss = [100, 2000]\n",
    "min_val_loss = [100, 2000]\n",
    "best_sim = [-1, 4000]\n",
    "sims = [0, 0]\n",
    "sim = [0, 0]\n",
    "colored, val_colored = [], []\n",
    "checkpoint_path = './models/MTN'\n",
    "epochs_no_improve = 0\n",
    "n_epochs_stop = 20\n",
    "min_val_losses = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch  0 / 2  \n",
      "Train loss: 54.0866, Val loss: 7.7997\n",
      "Hyperbolic\n",
      " epoch  1 / 2  \n",
      "Train loss: 54.1624, Val loss: 7.8209\n",
      "Hyperbolic\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_it = iter(trainloader)\n",
    "    val_it = iter(valloader)\n",
    "    \n",
    "    train_loss_SUM = 0\n",
    "    val_loss_SUM = 0\n",
    "    \n",
    "    \n",
    "    distributional_train_loss_SUM = 0\n",
    "    distributional_val_loss_SUM = 0\n",
    "    \n",
    "    hyperbolic_train_loss_SUM = 0\n",
    "    hyperbolic_val_loss_SUM = 0\n",
    "    \n",
    "    for batch_iteration in range(len(trainloader)):\n",
    "        x, labels, targets = next(train_it)\n",
    "        \n",
    "        ######################\n",
    "        ####### TRAIN ########\n",
    "        ######################\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        \n",
    "        output = model(x)\n",
    "        \n",
    "        output_dict = {'distributional' : output[0],\n",
    "                       'hyperbolic' : output[1]}\n",
    "        \n",
    "        distributional_train_loss = cosine_loss(output_dict['distributional'], \n",
    "                                                targets['distributional'])\n",
    "        \n",
    "        hyperbolic_train_loss, r = hyperbolic_loss(output_dict['hyperbolic'], \n",
    "                                                   targets['hyperbolic'], \n",
    "                                                   regul = regul)\n",
    "        \n",
    "        if weighted:\n",
    "            weights_train = get_weight(labels, labels_weights_train)\n",
    "            weights_train_SUM += torch.sum(weights).item()\n",
    "            train_loss = torch.sum((distributional_train_loss * llambda + hyperbolic_train_loss * (1 - llambda)) * weights)\n",
    "            distributional_train_loss_SUM += torch.sum(distributional_train_loss * llambda * weights).item()\n",
    "            hyperbolic_train_loss_SUM += torch.sum(hyperbolic_train_loss * (1 - llambda) * weights).item()\n",
    "\n",
    "        else:\n",
    "            train_loss = torch.sum(distributional_train_loss * llambda + hyperbolic_train_loss * (1 - llambda))\n",
    "            distributional_train_loss_SUM += torch.sum(distributional_train_loss * llambda).item()\n",
    "            hyperbolic_train_loss_SUM += torch.sum(hyperbolic_train_loss * (1 - llambda)).item()\n",
    "        \n",
    "        train_loss_SUM += train_loss.item()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    else:\n",
    "\n",
    "        ######################\n",
    "        ######## VAL #########\n",
    "        ######################\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()   \n",
    "            \n",
    "            for batch_iteration in range(len(valloader)):\n",
    "                x, labels, targets = next(val_it)\n",
    "    \n",
    "                output = model(x)\n",
    "                output_dict = {'distributional' : output[0],\n",
    "                               'hyperbolic' : output[1]}\n",
    "                \n",
    "                distributional_val_loss = cosine_loss(output_dict['distributional'],\n",
    "                                                      targets['distributional'])\n",
    "        \n",
    "                hyperbolic_val_loss, r = hyperbolic_loss(output_dict['hyperbolic'], \n",
    "                                                         targets['hyperbolic'], \n",
    "                                                         regul = [0, 0, 1])\n",
    "                \n",
    "                \n",
    "                if weighted:\n",
    "                    weights_val = get_weight(labels, labels_weights_val)\n",
    "                    weights_val_SUM += torch.sum(weights).item()   \n",
    "                    val_loss = torch.sum((distributional_val_loss * llambda + hyperbolic_val_loss * (1 - llambda)) * weights)\n",
    "                    distributional_val_loss_SUM += torch.sum(distributional_val_loss * llambda * weights).item()\n",
    "                    hyperbolic_val_loss_SUM += torch.sum(hyperbolic_val_loss * (1 - llambda) * weights).item()\n",
    "\n",
    "                else:\n",
    "                    val_loss = torch.sum(distributional_val_loss * llambda + hyperbolic_val_loss * (1 - llambda))\n",
    "                    distributional_val_loss_SUM += torch.sum(distributional_val_loss * llambda).item()\n",
    "                    hyperbolic_val_loss_SUM += torch.sum(hyperbolic_val_loss * (1 - llambda)).item()\n",
    "                val_loss_SUM += val_loss.item()    \n",
    "         \n",
    "        print('{:^15}'.format('epoch {:^3}/{:^3}'.format(epoch, epochs)))\n",
    "        print('{:^15}'.format('Train loss: {:.4f}, Val loss: {:.4f}'.format(train_loss_SUM/len(c.X_train), \n",
    "                                                                            val_loss_SUM/len(c.X_val)\n",
    "                                                                           )\n",
    "                             )\n",
    "             )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('MTNCI': conda)",
   "language": "python",
   "name": "python361064bitmtnciconda01fce39d24bd496eb8a955881010024a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
