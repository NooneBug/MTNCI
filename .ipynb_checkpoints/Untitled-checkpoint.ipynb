{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../figet-hyperbolic-space/')\n",
    "import figet\n",
    "from figet.model_utils import CharEncoder, SelfAttentiveSum, sort_batch_by_length\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabs': {'token': <figet.Dict.TokenDict at 0x7fdad06d3f10>,\n",
       "  'type': <figet.Dict.TypeDict at 0x7fda081efd00>,\n",
       "  'char': <figet.Dict.Dict at 0x7fda081efd30>},\n",
       " 'train': <figet.Dataset.Dataset at 0x7fda081efd60>,\n",
       " 'dev': <figet.Dataset.Dataset at 0x7fda081eff10>,\n",
       " 'test': <figet.Dataset.Dataset at 0x7fda012590d0>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lopez_data = torch.load('../figet-hyperbolic-space/data/prep/MTNCI/data.pt')\n",
    "lopez_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.add_argument(\"--emb_size\", default=300, type=int, help=\"Embedding size.\")\n",
    "# parser.add_argument(\"--char_emb_size\", default=50, type=int, help=\"Char embedding size.\")\n",
    "# parser.add_argument(\"--positional_emb_size\", default=25, type=int, help=\"Positional embedding size.\")\n",
    "# parser.add_argument(\"--context_rnn_size\", default=200, type=int, help=\"RNN size of ContextEncoder.\")\n",
    "# parser.add_argument(\"--attn_size\", default=100, type=int, help=\"Attention vector size.\")\n",
    "# parser.add_argument(\"--mention_dropout\", default=0.5, type=float, help=\"Dropout rate for mention\")\n",
    "# parser.add_argument(\"--context_dropout\", default=0.2, type=float, help=\"Dropout rate for context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CharEncoder(nn.Module):\n",
    "    def __init__(self, char_vocab, args):\n",
    "        super(CharEncoder, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        conv_dim_input = 100\n",
    "        filters = 5\n",
    "        self.char_W = nn.Embedding(char_vocab.size(), conv_dim_input, padding_idx=0)\n",
    "        self.conv1d = nn.Conv1d(conv_dim_input, args.char_emb_size, filters)  # input, output, filter_number\n",
    "\n",
    "    def forward(self, span_chars):\n",
    "        char_embed = self.char_W(span_chars).transpose(1, 2)  # [batch_size, char_embedding, max_char_seq]\n",
    "        conv_output = [self.conv1d(char_embed)]  # list of [batch_size, filter_dim, max_char_seq, filter_number]\n",
    "        conv_output = [F.relu(c) for c in conv_output]  # batch_size, filter_dim, max_char_seq, filter_num\n",
    "        cnn_rep = [F.max_pool1d(i, i.size(2)) for i in conv_output]  # batch_size, filter_dim, 1, filter_num\n",
    "        cnn_output = torch.squeeze(torch.cat(cnn_rep, 1), 2)  # batch_size, filter_num * filter_dim, 1\n",
    "        return cnn_output\n",
    "\n",
    "class MentionEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, char_vocab, args):\n",
    "        super(MentionEncoder, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.char_encoder = CharEncoder(char_vocab, args)\n",
    "        self.attentive_weighted_average = SelfAttentiveSum(args.emb_size, 1)\n",
    "        self.dropout = nn.Dropout(args.mention_dropout)\n",
    "\n",
    "    def forward(self, mentions, mention_chars, word_lut):\n",
    "        mention_embeds = word_lut(mentions)             # batch x mention_length x emb_size\n",
    "\n",
    "        weighted_avg_mentions, _ = self.attentive_weighted_average(mention_embeds)\n",
    "        char_embed = self.char_encoder(mention_chars)\n",
    "        output = torch.cat((weighted_avg_mentions, char_embed), 1)\n",
    "        return self.dropout(output).cuda()\n",
    "\n",
    "\n",
    "class ContextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.emb_size = args.emb_size\n",
    "        self.pos_emb_size = args.positional_emb_size\n",
    "        self.rnn_size = args.context_rnn_size\n",
    "        self.hidden_attention_size = 100\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.pos_linear = nn.Linear(1, self.pos_emb_size)\n",
    "        self.context_dropout = nn.Dropout(args.context_dropout)\n",
    "        self.rnn = nn.LSTM(self.emb_size + self.pos_emb_size, self.rnn_size, bidirectional=True, batch_first=True)\n",
    "        self.attention = SelfAttentiveSum(self.rnn_size * 2, self.hidden_attention_size) # x2 because of bidirectional\n",
    "\n",
    "    def forward(self, contexts, positions, context_len, word_lut, hidden=None):\n",
    "        \"\"\"\n",
    "        :param contexts: batch x max_seq_len\n",
    "        :param positions: batch x max_seq_len\n",
    "        :param context_len: batch x 1\n",
    "        \"\"\"\n",
    "        positional_embeds = self.get_positional_embeddings(positions)   # batch x max_seq_len x pos_emb_size\n",
    "        ctx_word_embeds = word_lut(contexts)                            # batch x max_seq_len x emb_size\n",
    "        ctx_embeds = torch.cat((ctx_word_embeds, positional_embeds), 2)\n",
    "\n",
    "        ctx_embeds = self.context_dropout(ctx_embeds)\n",
    "\n",
    "        rnn_output = self.sorted_rnn(ctx_embeds, context_len)\n",
    "\n",
    "        return self.attention(rnn_output)\n",
    "\n",
    "    def get_positional_embeddings(self, positions):\n",
    "        \"\"\" :param positions: batch x max_seq_len\"\"\"\n",
    "        pos_embeds = self.pos_linear(positions.view(-1, 1))                     # batch * max_seq_len x pos_emb_size\n",
    "        return pos_embeds.view(positions.size(0), positions.size(1), -1)        # batch x max_seq_len x pos_emb_size\n",
    "\n",
    "    def sorted_rnn(self, ctx_embeds, context_len):\n",
    "        sorted_inputs, sorted_sequence_lengths, restoration_indices = sort_batch_by_length(ctx_embeds, context_len)\n",
    "        packed_sequence_input = pack(sorted_inputs, sorted_sequence_lengths, batch_first=True)\n",
    "        packed_sequence_output, _ = self.rnn(packed_sequence_input, None)\n",
    "        unpacked_sequence_tensor, _ = unpack(packed_sequence_output, batch_first=True)\n",
    "        return unpacked_sequence_tensor.index_select(0, restoration_indices)\n",
    "\n",
    "\n",
    "\n",
    "# def get_shimaoka(input, mention_encoder, context_encoder):\n",
    "#     contexts, positions, context_len = input[0], input[1], input[2]\n",
    "#     mentions, mention_chars = input[3], input[4]\n",
    "#     type_indexes = input[5]\n",
    "\n",
    "#     mention_vec = mention_encoder(mentions, mention_chars, self.word_lut)\n",
    "#     context_vec, attn = context_encoder(contexts, positions, context_len, self.word_lut)\n",
    "\n",
    "#     input_vec = torch.cat((mention_vec, context_vec), dim=1)\n",
    "#     return input_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MTNCI import MTNCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argClass():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.emb_size = 300 \n",
    "        self.char_emb_size = 50 \n",
    "        self.positional_emb_size = 25 \n",
    "        self.context_rnn_size = 200\n",
    "        self.attn_size = 100\n",
    "        self.mention_dropout = 0.5\n",
    "        self.context_dropout = 0.5\n",
    "\n",
    "\n",
    "args = {'emb_size': 300, 'char_emb_size': 50, 'positional_emb_size': 25, 'context_rnn_size':200,\n",
    "        'attn_size': 100, 'mention_dropout' : 0.5, 'context_dropout': 0.5}\n",
    "args = argClass(args)\n",
    "vocabs = lopez_data['vocabs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShimaokaMTNCI(MTNCI):\n",
    "    \n",
    "    def __init__(self, argss, vocabs, device, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        CHAR_VOCAB = 'char'        \n",
    "        self.word_lut = nn.Embedding(vocabs[\"token\"].size_of_word2vecs(), \n",
    "                                     argss.emb_size,\n",
    "                                     padding_idx=0).cuda()\n",
    "        \n",
    "        self.mention_encoder = MentionEncoder(vocabs[CHAR_VOCAB], argss).cuda()\n",
    "        self.context_encoder = ContextEncoder(argss).cuda()\n",
    "        self.feature_len = argss.context_rnn_size * 2 + argss.emb_size + argss.char_emb_size\n",
    "    \n",
    "    def init_params(self, word2vec):\n",
    "        self.word_lut.weight.data.copy_(word2vec)\n",
    "        self.word_lut.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, input):\n",
    "        contexts, positions, context_len = input[0], input[1].double(), input[2]\n",
    "        mentions, mention_chars = input[3], input[4]\n",
    "        type_indexes = input[5]\n",
    "                \n",
    "        mention_vec = self.mention_encoder(mentions, mention_chars, self.word_lut)\n",
    "        \n",
    "        context_vec, attn = self.context_encoder(contexts, positions, context_len, self.word_lut)\n",
    "\n",
    "        input_vec = torch.cat((mention_vec, context_vec), dim=1)\n",
    "        \n",
    "        return super().forward(input_vec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIMAOKA_OUT = args.context_rnn_size * 2 + args.emb_size + args.char_emb_size\n",
    "\n",
    "out_spec = [{'manifold':'euclid', 'dim':[64, 10]},\n",
    "                {'manifold':'poincare', 'dim':[128, 128, 10]}]\n",
    "\n",
    "m = ShimaokaMTNCI(args, vocabs, device, \n",
    "                  input_d=SHIMAOKA_OUT,\n",
    "                out_spec = out_spec,\n",
    "                dims = [512, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size, key):\n",
    "    dataset = data[key]\n",
    "    dataset.set_batch_size(batch_size)\n",
    "    return dataset\n",
    "\n",
    "test = get_dataset(lopez_data, 1024, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = torch.load(\"../figet-hyperbolic-space/data/prep/MTNCI/word2vec.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.init_params(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 397,   39,    1,  ...,    0,    0,    0],\n",
      "        [  51,  395,   10,  ...,    0,    0,    0],\n",
      "        [  51,  395,  111,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  51,  395,   11,  ...,    0,    0,    0],\n",
      "        [1998, 4447,  405,  ...,    0,    0,    0],\n",
      "        [2674,  186,  200,  ...,    0,    0,    0]], device='cuda:0')\n",
      "tensor([[10.,  9.,  8.,  ..., -1., -1., -1.],\n",
      "        [10.,  9.,  8.,  ..., -1., -1., -1.],\n",
      "        [10.,  9.,  8.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [10.,  9.,  8.,  ..., -1., -1., -1.],\n",
      "        [10.,  9.,  8.,  ..., -1., -1., -1.],\n",
      "        [10.,  9.,  8.,  ..., -1., -1., -1.]], device='cuda:0')\n",
      "tensor([18, 16, 16,  ..., 18, 16, 18], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-cafe2f9a5e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MTNCI_figet/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-fc644e8ba90c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mcontext_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_lut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0minput_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmention_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MTNCI_figet/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-8688e814265a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, contexts, positions, context_len, word_lut, hidden)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mctx_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mrnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-8688e814265a>\u001b[0m in \u001b[0;36msorted_rnn\u001b[0;34m(self, ctx_embeds, context_len)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msorted_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0msorted_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_sequence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_batch_by_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpacked_sequence_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_sequence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mpacked_sequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0munpacked_sequence_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pack' is not defined"
     ]
    }
   ],
   "source": [
    "m(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MTNCI_figet]",
   "language": "python",
   "name": "conda-env-MTNCI_figet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
